import argparse
from typing import Dict, Literal, List, Any, Tuple, Union
import pandas as pd
from pathlib import Path
from sklearn.inspection import permutation_importance
from sklearn.metrics import f1_score

from ASCOPD_model import DockerModelWrapper
from utils import is_between, load_csv, store_json


RANDOM_STATE = 2025
NUMBER_REPEATS = 5
CATEGORICAL_COLUMNS = ["Sex"]


def pass_checks(
    tabular_data: pd.DataFrame,
    actual_target: pd.DataFrame,
    target_col: str,
) -> bool:
    """Validate that input DataFrames and required columns pass basic checks.

    Args:
        tabular_data (pd.DataFrame): The main tabular dataset.
        actual_target (pd.DataFrame): DataFrame containing the actual target values.
        target_col (str): The name of the target column that must exist in `actual_target`.

    Returns:
        bool: True if all checks pass, otherwise False.

    Checks performed:
        1. None of the inputs are None.
        2. None of the DataFrames are empty.
        3. DataFrames have the same length.
        4. `target_col` exists in `actual_target`.
    """

    if (tabular_data is None) or (actual_target is None):
        return False

    if tabular_data.empty or actual_target.empty:
        return False

    if not len(tabular_data) == len(actual_target):
        return False

    if target_col not in actual_target.columns:
        return False

    return True


def select_method(sens: float) -> Literal["feature_permutation", "baseline_occlusion"]:
    """
    Select an explainability method based on sensitivity level.
    Values < 0.5 -> "feature_permutation".
    Values >= 0.5 -> "baseline_occlusion".

    Args:
        sens (float): Sensitivity parameter in the range [0, 1].

    Returns:
        Literal["feature_permutation", "baseline_occlusion"]:
            The name of the selected explainability method.
    """

    return "feature_permutation" if sens < 0.5 else "baseline_occlusion"


def configure_model(
    target: Literal["VenDep", "ARF", "Mortality"],
) -> DockerModelWrapper:
    """
    Initialize and configure the Docker model wrapper for a specific target column.

    Args:
        target (Literal["VenDep", "ARF", "Mortality"]): The target column
            for which predictions will be generated by the Docker model.

    Returns:
        DockerModelWrapper: An instance of the Docker model wrapper configured.
    """

    return DockerModelWrapper(target=target)


def feature_permutation(
    X: pd.DataFrame, y: pd.DataFrame, model: DockerModelWrapper
) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    Compute feature importance using permutation importance with a Dockerized model.
    This function wraps `sklearn.inspection.permutation_importance` to estimate how much
    each feature contributes to model performance. It evaluates feature importance
    with F1 scoring, and aggregates results across multiple repeats.

    Args:
        X (pd.DataFrame): Input features used for prediction.
        y (pd.DataFrame): True target values corresponding to `X`.
        model (DockerModelWrapper): Dockerized model wrapper.

    Returns:
        Tuple[List[Dict[str, Any]], Dict[str, Any]]:
            - A list of dictionaries with feature names and their mean permutation importance.
              Example: `[{"Feature": "age", "Permutation_Importance": 0.12}, ...]`
            - A dictionary containing the full permutation importance results from sklearn,
              including per-repeat scores (`importances`), mean (`importances_mean`),
              and standard deviation (`importances_std`).
    """

    importance_results = permutation_importance(
        estimator=model,
        X=X,
        y=y,
        scoring="f1",
        n_repeats=NUMBER_REPEATS,
        random_state=RANDOM_STATE,
        n_jobs=-1,
    )

    summary_results = list(
        map(
            lambda col, imp: {"Feature": col, "Permutation_Importance": imp},
            X.columns,
            importance_results.importances_mean,
        )
    )

    importance_results = dict(importance_results)
    importance_results["features"] = importance_results.columns.to_list()

    return summary_results, importance_results


def classification_score(
    true: Union[pd.Series, list], pred: Union[pd.Series, list]
) -> float:
    """
    Compute the F1 score for a binary or multiclass classification task.

    Args:
        true (Union[pd.Series, list]): Ground truth labels.
        pred (Union[pd.Series, list]): Predicted labels.

    Returns:
        float: F1 score between true and predicted labels.
    """

    return f1_score(y_true=true, y_pred=pred)


def feature_occlusion(
    X: pd.DataFrame, y: pd.DataFrame, model: DockerModelWrapper
) -> List[Dict[str, Any]]:
    """
    Compute feature importance using baseline occlusion for a DockerModelWrapper.
    For each feature in `X`, the column is replaced with a baseline value:
    - Categorical columns use the mode.
    - Numerical columns use the mean.
    The model predictions on the modified dataset are compared to the original predictions
    using the F1 score. The drop in score is taken as the feature's occlusion importance.

    Args:
        X (pd.DataFrame): Input features used for prediction.
        y (pd.DataFrame): True target labels corresponding to `X`.
        model (DockerModelWrapper): Dockerized model wrapper.

    Returns:
        List[Dict[str, Any]]: List of dictionaries with each feature name and its occlusion importance.
        Example: `[{"Feature": "age", "Occlusion_Importance": 0.12}, ...]`
    """

    baseline_pred = model.predict(X)
    baseline_score = classification_score(true=y, pred=baseline_pred)

    summary_results = []

    for col in X.columns:
        X_occlusion = X.copy()

        replace_value = (
            X[col].mode().iloc[0] if col in CATEGORICAL_COLUMNS else X[col].median()
        )
        X_occlusion[col] = replace_value

        occlusion_pred = model.predict(X_occlusion)
        occlusion_score = classification_score(true=y, pred=occlusion_pred)

        occlusion_importance = baseline_score - occlusion_score

        summary_results.append(
            {"Feature": col, "Occlusion_Importance": occlusion_importance}
        )

    return summary_results


def run_explainability_analysis(
    tabular_data: Union[str, Path],
    actual_target: Union[str, Path],
    target_col: str,
    output_dir: Union[str, Path],
    sensitivity: float,
) -> List[Dict[str, Any]]:
    """
    Runs an explainability analysis on tabular data using either permutation importance
    or baseline occlusion, depending on the specified sensitivity.
    The function:
        1. Loads tabular feature data and actual target labels.
        2. Checks data consistency.
        3. Selects the explainability method based on sensitivity.
           - Low sensitivity (<0.5): Feature permutation
           - High sensitivity (>=0.5): Baseline occlusion
        4. Configures the Dockerized model.
        5. Computes feature importances.
        6. Stores detailed results (if applicable) and summary results as JSON.

    Args:
        tabular_data (Union[str, Path]): Path to CSV file containing input features.
        actual_target (Union[str, Path]): Path to CSV file containing true target labels.
        target_col (str): Name of the target column for the model.
        output_dir (Path): Directory where results will be stored.
        sensitivity (float): Sensitivity parameter (0 to 1) controlling the method selection.
    """

    # Init results
    results = [{}]
    output_dir.mkdir(parents=True, exist_ok=True)

    # Load data
    tabular_data = load_csv(tabular_data)
    actual_target = load_csv(actual_target)

    if not pass_checks(tabular_data, actual_target, target_col):
        raise ValueError(f"Inconsistent data provided. Check again the data provided.")

    method = select_method(sensitivity)
    print(f"Using method: {method} based on sensitivity: {sensitivity}")

    # Perform analysis
    model = configure_model(target_col)

    if method == "feature_permutation":
        results, detailed_results = feature_permutation(
            X=tabular_data, y=actual_target, model=model
        )
        store_json(
            data=detailed_results,
            path=output_dir.joinpath(f"{method}_analysis_detailed_results.json"),
        )
    elif method == "baseline_occlusion":
        results = feature_occlusion(X=tabular_data, y=actual_target, model=model)

    # Store summary results to output dir
    store_json(data=results, path=output_dir.joinpath(f"{method}_analysis.json"))


def main():
    parser = argparse.ArgumentParser(
        description="Analyze and explain feature importance"
    )

    parser.add_argument(
        "--tabular_data", required=True, help="Path to tabular data CSV file"
    )
    parser.add_argument(
        "--actual_target",
        required=True,
        help="Path to groundtruth target data CSV file",
    )
    parser.add_argument(
        "--target_col",
        required=True,
        choices=["VenDep", "ARF", "Mortality"],
        help="Name of target column name in actual and predicted target CSV file",
    )
    parser.add_argument(
        "--output",
        default="output",
        help="Output dir for results JSON files",
    )

    parser.add_argument(
        "--sensitivity",
        type=float,
        default=0.7,
        help="Sensitivity value (0-1): <0.5 uses permutation feature importance, >=0.5 uses baseline occlusion",
    )
    args = parser.parse_args()

    if not is_between(x=args.sensitivity):
        raise ValueError("Sensitivity must be between 0 and 1")

    run_explainability_analysis(
        tabular_data=args.tabular_data,
        actual_target=args.actual_target,
        target_col=args.target_col,
        output_dir=Path(args.output),
        sensitivity=args.sensitivity,
    )


if __name__ == "__main__":
    main()
